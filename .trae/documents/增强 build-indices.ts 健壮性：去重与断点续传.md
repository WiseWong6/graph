# 增强 build-indices.ts 健壮性计划

## 核心目标
解决索引构建过程中的意外中断问题，实现**去重**和**断点续传**，确保脚本可以安全地反复运行，而不会产生重复数据或从头开始。

## 故障分析 (Post-Mortem)
用户反馈脚本在第4批后"自己中断了"。可能原因：
1.  **内存溢出 (OOM)**：Node.js 默认堆内存限制（约 1.5GB）可能不足以支撑 LlamaIndex 在内存中维护越来越大的索引结构。
2.  **I/O 错误**：文件句柄耗尽或写入超时。
3.  **脏数据**：某篇文章内容过大或格式异常导致 Embedding 模型崩溃。

## 解决方案 (Linus Style)

### 1. 实现"智能跳过" (去重)
在处理每一批数据前，先检查该数据是否已经存在于索引中。
*   **机制**：加载现有的 `docStore`，获取所有已索引的 `docId`。
*   **策略**：如果一个文档的 ID（或哈希）已存在，直接跳过 Embedding 和 Insert 步骤。
*   **收益**：如果脚本中断，下次运行时前 4 批会自动瞬间跳过，直接从第 5 批开始。

### 2. 优化内存管理
目前的逻辑是 `index.insert(doc)`，这会将所有新旧节点都保留在内存对象中。
*   **改进**：每处理完一批，尝试释放不必要的引用（虽然 JS GC 是自动的，但我们可以减少全局变量的持有）。
*   **强制 GC**：在启动脚本时建议增加 `--max-old-space-size=4096`。

### 3. 错误捕获与跳过
目前是 `Promise.all` 并行处理一批。如果其中一条失败，整批都会挂掉。
*   **改进**：改为 `Promise.allSettled` 或者在单个 insert 内部 `try-catch`，允许个别文档失败而不中断整个进程。

## 具体实施步骤

1.  **修改 `buildIndexInBatches`**：
    *   在循环开始前，尝试从磁盘加载现有的 `VectorStoreIndex`（如果存在）。
    *   如果加载成功，提取所有已存在的 `refDocId`。
    *   在处理每个 batch 时，过滤掉已存在的文档。
    *   如果 batch 为空（全跳过），直接 log 并 continue。

2.  **增强 ID 生成**：
    *   确保每个 Document 都有确定的、基于内容的 ID（如 MD5 hash 或文件名+索引），而不是随机 UUID。这样才能确保跨进程的去重有效。
    *   目前文章库使用的是文件名+索引，金句库有 ID，需要确认。

3.  **增加单条容错**：
    *   包裹 `index.insert` 在 try-catch 中，打印错误但继续执行。

## 预期结果
*   **幂等性**：脚本可以运行 N 次，结果不变。
*   **可恢复性**：中断后再次运行，自动从断点处继续。
*   **稳定性**：个别坏数据不会炸掉整个任务。
