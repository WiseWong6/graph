## 修复目标
确保 Humanize 节点的流式输出（thinking + response）完全结束后，再输出 Prompts/Images 节点的内容。

## 根本原因
1. `shouldBufferLog()` 只缓冲 `console.log()`，流式输出使用 `process.stdout.write()` 绕过了检查
2. Humanize 和 Prompts 并行执行时，两个节点的流式输出会交错
3. Prompts 节点的 `outputCoordinator.defer()` 只延迟日志输出，无法延迟流式输出

## 修复方案

### 1. 扩展 OutputCoordinator（llm-output.ts）
- 添加节点级别的流状态追踪：`nodeStreams: Map<string, number>`
- 添加 `beginNodeStream(nodeId)` 和 `endNodeStream(nodeId)` 方法
- 添加 `shouldSuppressOutput(nodeId)` 方法：检查是否有其他节点正在流式输出
- 修改 `defer()` 方法：支持按节点 ID 过滤输出

### 2. 修改 openai-compat.ts 的流式输出
- `callThinkingStream()` 接受 `nodeId` 参数
- 在流式输出前调用 `outputCoordinator.shouldSuppressOutput(nodeId)`
- 如果返回 true，完全抑制流式输出（thinking 和 response 都不输出）
- 同时更新 `suppressStreaming` 逻辑：节点级抑制优先于配置级抑制

### 3. 更新 LLMClient 接口（llm-client.ts）
- `call()` 方法接受 `nodeId` 参数
- 传递给 adapter

### 4. 更新 llm-runner.ts
- `callLLMWithFallback()` 接受 `nodeId` 参数
- 传递给 LLMClient

### 5. 更新节点代码
- 09_humanize.node.ts: 调用时传递 `nodeId: "09_humanize"`
- 10_prompts.node.ts: 调用时传递 `nodeId: "10_prompts"`

### 6. step-cli.ts 确保输出顺序
- Humanize 完成后，输出所有缓冲的 Prompts/Images 日志
- 利用现有的 `tracker.deferredCompletions` 和 `deferredLogs` 机制

## 预期效果
- Humanize 流式输出完整显示（thinking + response）
- Prompts/Images 的所有输出延迟到 Humanize 完成后
- 输出清晰有序，无交错