// LLM client utility
// Provides a unified interface for interacting with different LLM providers
//
// "Bad programmers worry about the code. Good programmers worry about data structures."
// This module abstracts provider differences behind a clean data structure.

import OpenAI from "openai";
import type { LLMNodeConfig } from "../config/llm.js";

/**
 * Simple async mutex to prevent concurrent stdout writes
 * Ensures streaming output from parallel nodes doesn't interleave
 */
class AsyncMutex {
  private locked = false;
  private queue: Array<(value: void) => void> = [];

  async acquire(): Promise<() => void> {
    while (this.locked) {
      await new Promise<void>(resolve => this.queue.push(resolve));
    }
    this.locked = true;
    return () => this.release();
  }

  private release(): void {
    this.locked = false;
    const resolve = this.queue.shift();
    if (resolve) resolve();
  }
}

// Module-level mutex for all stdout operations
const stdoutMutex = new AsyncMutex();

// DeepSeek-specific response extensions
// The DeepSeek Reasoner model returns reasoning_content and reasoning_tokens
interface DeepSeekCompletionMessage extends OpenAI.ChatCompletionMessage {
  reasoning_content?: string;
}

interface DeepSeekCompletionUsage extends OpenAI.CompletionUsage {
  reasoning_tokens?: number;
}

// Unified LLM call options - request parameters
export interface LLMCallOptions {
  prompt: string;
  systemMessage?: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
}

// Unified LLM response - normalized output
export interface LLMResponse {
  text: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// Stream chunk - partial text from streaming
export interface StreamChunk {
  text: string;
  done: boolean;
}

/**
 * Unified LLM client supporting multiple providers
 *
 * Design principle: Provider differences are implementation details,
 * hidden behind a single clean interface.
 */
export class LLMClient {
  constructor(private config: LLMNodeConfig) {}

  /**
   * Main entry point - routes to appropriate provider implementation
   * No special cases here, just data-driven dispatch
   */
  async call(options: LLMCallOptions): Promise<LLMResponse> {
    const provider = this.config.provider;

    switch (provider) {
      case "doubao":
      case "openai":
      case "deepseek":
        return await this.callOpenAICompatible(options);
      case "anthropic":
        return await this.callAnthropic(options);
      default:
        // Exhaustiveness check - if this compiles, we handled all providers
        const _exhaustive: never = provider;
        throw new Error(`Unsupported provider: ${_exhaustive}`);
    }
  }

  /**
   * Streaming version - yields text chunks as they arrive
   * Returns AsyncGenerator for use with for-await loops
   */
  async *stream(options: LLMCallOptions): AsyncGenerator<StreamChunk> {
    const provider = this.config.provider;

    switch (provider) {
      case "doubao":
      case "openai":
      case "deepseek":
        yield* this.streamOpenAICompatible(options);
        break;
      case "anthropic":
        throw new Error("Anthropic streaming not yet implemented");
      default:
        const _exhaustive: never = provider;
        throw new Error(`Unsupported provider: ${_exhaustive}`);
    }
  }

  /**
   * OpenAI-compatible streaming implementation
   */
  private async *streamOpenAICompatible(options: LLMCallOptions): AsyncGenerator<StreamChunk> {
    const apiKey = this.getApiKey(
      this.config.api_key_env ||
      (this.config.provider === "deepseek" ? "DEEPSEEK_API_KEY" :
       this.config.provider === "doubao" ? "DOUBAO_API_KEY" :
       "OPENAI_API_KEY")
    );
    const baseURL = this.config.base_url ||
      (this.config.provider === "deepseek" ? "https://api.deepseek.com" :
       this.config.provider === "doubao" ? "https://ark.cn-beijing.volces.com/api/v3" :
       "https://api.openai.com/v1");

    const client = new OpenAI({ apiKey, baseURL });

    const messages: OpenAI.ChatCompletionMessageParam[] = [];
    if (options.systemMessage) {
      messages.push({ role: "system", content: options.systemMessage });
    }
    messages.push({ role: "user", content: options.prompt });

    const stream = await client.chat.completions.create({
      model: this.config.model,
      messages,
      max_tokens: options.maxTokens || this.config.max_tokens || 1024,
      temperature: options.temperature || this.config.temperature || 0.7,
      stream: true,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        yield { text: content, done: false };
      }
      if (chunk.choices[0]?.finish_reason) {
        yield { text: "", done: true };
        break;
      }
    }
  }

  /**
   * OpenAI-compatible implementation (non-streaming)
   * Used by both OpenAI and DeepSeek (which is API-compatible)
   *
   * This is the "happy path" - standard REST API, standard response format
   *
   * DeepSeek Reasoner support:
   * - Streams reasoning_content when available (thinking process)
   * - Configurable timeout to prevent "terminated" errors
   */
  private async callOpenAICompatible(options: LLMCallOptions): Promise<LLMResponse> {
    const apiKey = this.getApiKey(
      this.config.api_key_env ||
      (this.config.provider === "deepseek" ? "DEEPSEEK_API_KEY" :
       this.config.provider === "doubao" ? "DOUBAO_API_KEY" :
       "OPENAI_API_KEY")
    );
    const baseURL = this.config.base_url ||
      (this.config.provider === "deepseek" ? "https://api.deepseek.com" :
       this.config.provider === "doubao" ? "https://ark.cn-beijing.volces.com/api/v3" :
       "https://api.openai.com/v1");

    // Calculate timeout: provider-specific multipliers for reasoning models
    // DeepSeek Reasoner and Doubao Thinking may take up to 60s for thinking alone
    const isReasoningProvider = this.config.provider === "deepseek" ||
      (this.config.provider === "doubao" && this.config.thinking?.type !== "disabled");
    const timeoutMs = isReasoningProvider
      ? (this.config.timeout || 120000) * 2
      : (this.config.timeout || 120000);

    const client = new OpenAI({
      apiKey,
      baseURL,
      timeout: timeoutMs,
    });

    // Build messages array - standard format shared by OpenAI-compatible APIs
    const messages: OpenAI.ChatCompletionMessageParam[] = [];
    if (options.systemMessage) {
      messages.push({ role: "system", content: options.systemMessage });
    }
    messages.push({ role: "user", content: options.prompt });

    // DeepSeek Reasoner ä½¿ç”¨æµå¼è¾“å‡ºï¼ˆä»… reasoner æ¨¡å‹ï¼‰
    const isDeepSeekReasoner = this.config.provider === "deepseek" && this.config.model.includes("reasoner");

    // Doubao æ·±åº¦æ€è€ƒæ¨¡å‹ä¹Ÿä½¿ç”¨æµå¼è¾“å‡ºï¼ˆthinking.type !== "disabled"ï¼‰
    const isDoubaoThinking = this.config.provider === "doubao" && this.config.thinking?.type !== "disabled";

    if (isDeepSeekReasoner || isDoubaoThinking) {
      return await this.callDeepSeekStreaming(client, messages, options);
    }

    const completion = await client.chat.completions.create({
      model: this.config.model,
      messages,
      max_tokens: options.maxTokens || this.config.max_tokens || 1024,
      temperature: options.temperature || this.config.temperature || 0.7,
    });

    // Handle DeepSeek Reasoner's reasoning_content (thinking process)
    const response = completion.choices[0].message as DeepSeekCompletionMessage;
    if (response.reasoning_content) {
      console.log(`[LLMClient] ğŸ’­ DeepSeek Reasoning:\n${response.reasoning_content}`);
    }

    // Normalize response to unified format
    const usage = completion.usage as DeepSeekCompletionUsage | undefined;
    const reasoningTokens = usage?.reasoning_tokens || 0;
    return {
      text: response.content || "",
      usage: {
        prompt_tokens: completion.usage?.prompt_tokens || 0,
        completion_tokens: (completion.usage?.completion_tokens || 0) + reasoningTokens,
        total_tokens: (completion.usage?.total_tokens || 0) + reasoningTokens,
      },
    };
  }

  /**
   * DeepSeek Reasoner / Doubao Thinking æµå¼è¾“å‡º
   * æ”¯æŒè¿”å› reasoning_content å­—æ®µçš„æ¨¡å‹
   * é€å­—æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå†…å®¹
   * ä½¿ç”¨äº’æ–¥é”é˜²æ­¢å¹¶è¡Œæ‰§è¡Œæ—¶è¾“å‡ºäº¤é”™
   */
  private async callDeepSeekStreaming(
    client: OpenAI,
    messages: OpenAI.ChatCompletionMessageParam[],
    options: LLMCallOptions
  ): Promise<LLMResponse> {
    // æ„å»ºæµå¼è¯·æ±‚å‚æ•°
    const streamParams: any = {
      model: this.config.model,
      messages,
      max_tokens: options.maxTokens || this.config.max_tokens || 4096,
      temperature: options.temperature || this.config.temperature || 0.7,
      stream: true,
    };

    // Doubao æ·±åº¦æ€è€ƒï¼šé€šè¿‡ extra_body ä¼ é€’ thinking å‚æ•°
    if (this.config.provider === "doubao" && this.config.thinking) {
      streamParams.extra_body = {
        thinking: this.config.thinking
      };
    }

    const stream = await client.chat.completions.create(streamParams) as unknown as AsyncIterable<OpenAI.ChatCompletionChunk>;

    // åˆ¤æ–­æ¨¡å‹ç±»å‹ï¼ˆç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼‰
    const isDeepSeek = this.config.provider === "deepseek";
    const providerName = isDeepSeek ? "DeepSeek" : "Doubao";

    let reasoningContent = "";
    let responseContent = "";
    let inReasoning = false;

    console.log(`[LLMClient] ğŸ’­ ${providerName} Thinking:`);

    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta as any; // DeepSeek/Doubao ç‰¹æœ‰å­—æ®µ

      // å¤„ç†æ¨ç†å†…å®¹ï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
      if (delta?.reasoning_content) {
        const text = delta.reasoning_content;
        reasoningContent += text;
        // ä½¿ç”¨äº’æ–¥é”ä¿æŠ¤ stdoutï¼Œé˜²æ­¢å¹¶è¡Œè¾“å‡ºäº¤é”™
        const release = await stdoutMutex.acquire();
        try {
          process.stdout.write(text);
        } finally {
          release();
        }
        inReasoning = true;
      }

      // å¤„ç†å“åº”å†…å®¹ï¼ˆæœ€ç»ˆå›ç­”ï¼‰
      if (delta?.content) {
        if (inReasoning) {
          console.log(); // æ€è€ƒç»“æŸï¼Œæ¢è¡Œ
          console.log(`[LLMClient] âœï¸  ${providerName} Response:`);
          inReasoning = false;
        }
        const text = delta.content;
        responseContent += text;
        // ä½¿ç”¨äº’æ–¥é”ä¿æŠ¤ stdoutï¼Œé˜²æ­¢å¹¶è¡Œè¾“å‡ºäº¤é”™
        const release = await stdoutMutex.acquire();
        try {
          process.stdout.write(text);
        } finally {
          release();
        }
      }
    }

    // ç¡®ä¿æ¢è¡Œï¼ˆå¦‚æœè¾“å‡ºäº†å†…å®¹ï¼‰
    if (inReasoning || responseContent.length > 0) {
      console.log();
    }

    // ç¡®ä¿è¾“å‡ºå®Œå…¨åˆ·æ–°
    process.stdout.write("");

    // è®¡ç®—ä½¿ç”¨é‡ï¼ˆä¼°ç®—ï¼‰
    const reasoningTokens = reasoningContent.length; // ç²—ç•¥ä¼°ç®—
    const responseTokens = responseContent.length;

    return {
      text: responseContent,
      usage: {
        prompt_tokens: 0, // æµå¼å“åº”ä¸è¿”å› prompt_tokens
        completion_tokens: reasoningTokens + responseTokens,
        total_tokens: reasoningTokens + responseTokens,
      },
    };
  }

  /**
   * Anthropic implementation
   *
   * TODO: Implement in phase 2.5
   * Requires @anthropic-ai/sdk package which is not yet installed
   */
  private async callAnthropic(_options: LLMCallOptions): Promise<LLMResponse> {
    throw new Error(
      "Anthropic provider not yet implemented. " +
      "Install @anthropic-ai/sdk and uncomment the implementation."
    );

    // Implementation sketch (requires @anthropic-ai/sdk):
    /*
    const Anthropic = require("@anthropic-ai/sdk");
    const client = new Anthropic({
      apiKey: this.getApiKey(this.config.api_key_env || "ANTHROPIC_API_KEY"),
    });

    const message = await client.messages.create({
      model: this.config.model,
      max_tokens: options.maxTokens || this.config.max_tokens || 1024,
      system: options.systemMessage,
      messages: [{ role: "user", content: options.prompt }],
    });

    return {
      text: message.content[0].type === "text" ? message.content[0].text : "",
      usage: {
        prompt_tokens: message.usage.input_tokens,
        completion_tokens: message.usage.output_tokens,
        total_tokens: message.usage.input_tokens + message.usage.output_tokens,
      },
    };
    */
  }

  /**
   * Get API key from environment
   *
   * Fails fast if key is missing - better to error early than to fail
   * midway through a request with a cryptic authentication error.
   */
  private getApiKey(envVar: string): string {
    const apiKey = process.env[envVar];
    if (!apiKey) {
      throw new Error(
        `Environment variable ${envVar} is not set. ` +
        `Please set it in your .env file or environment.`
      );
    }
    return apiKey;
  }
}
